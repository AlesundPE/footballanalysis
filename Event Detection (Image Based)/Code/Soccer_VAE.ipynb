{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soccer_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6qCz6WNvNlk"
      },
      "source": [
        "import os\n",
        "train_address = \"Train\"\n",
        "\n",
        "if not os.path.exists(train_address):\n",
        "  os.makedirs(train_address)\n",
        "  !mkdir images\n",
        "  !pip install gdown\n",
        "  !gdown https://drive.google.com/uc?id=1PYtBOmTYGVugPN1bMg51UCB1lf_cY_m-\n",
        "  !unzip Center.zip\n",
        "  !mv Center event\n",
        "  !gdown https://drive.google.com/uc?id=1jEXejjSuRyPkGCobUgf3uHobXhhzzT5h\n",
        "  !unzip Corner.zip\n",
        "  !mv Corner event\n",
        "  !gdown https://drive.google.com/uc?id=1b571zn4tTlPCFsTQlEp1MHxMEr-nVlQy\n",
        "  !unzip Free-Kick.zip\n",
        "  !mv Free-Kick event\n",
        "  !gdown https://drive.google.com/uc?id=1FhnLqubUW_OS5V3KjEbIpDC2rjbkKiVn\n",
        "  !unzip Left.zip\n",
        "  !mv Left event\n",
        "  !gdown https://drive.google.com/uc?id=1PBdkrqIwGorqRCTWIUoiWC8oKxCMsfnH\n",
        "  !unzip Penalty.zip\n",
        "  !mv Penalty event\n",
        "  !gdown https://drive.google.com/uc?id=1ouS9-7c3eAFzMTkd1W9PLPBDkLjLIZGS\n",
        "  !unzip Red-Cards.zip\n",
        "  !mv Red-Cards event\n",
        "  !gdown https://drive.google.com/uc?id=1pEjGFLmCAqK8blt2KSeUTtVDy6FQoZeR\n",
        "  !unzip Right.zip\n",
        "  !mv Right event\n",
        "  !gdown https://drive.google.com/uc?id=17ZIFSj4gkfKjSz-GP9MF_vRU1nhpQLOd\n",
        "  !unzip Tackle.zip\n",
        "  !mv Tackle event\n",
        "  !gdown https://drive.google.com/uc?id=1hpmmX2LwBM4OPrTazNTkEEOONeoGCpHb\n",
        "  !unzip To-Subtitue.zip\n",
        "  !mv \"To Subtitue\" event\n",
        "  !gdown https://drive.google.com/uc?id=1HeI8akLuhJwJ9RWpYpAtHW5MKdgipFnm\n",
        "  !unzip Yellow-Cards.zip\n",
        "  !mv Yellow-Cards event\n",
        "  !gdown https://drive.google.com/uc?id=14vP84hfX-dmuW07dnYSAFg_gZNFnIDuG\n",
        "  !unzip Event.zip\n",
        "  !mv Event SoccerEvent\n",
        "  !gdown https://drive.google.com/uc?id=1XNwuiv1qGcNOwUoGZShZKbaJXn9N2_YV\n",
        "  !unzip Others.zip\n",
        "  !gdown https://drive.google.com/uc?id=1RiFOFQ9HSpzKgK3A_vbL05yIty5J6R-o\n",
        "  !unzip Soccer.zip\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfTBNiIrDyon"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import cv2\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUjiKaSgDyrW"
      },
      "source": [
        "from imutils import paths\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "filelist = \"event/\"\n",
        "imagePaths = sorted(list(paths.list_images(filelist)))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "data =[]\n",
        "print(len(imagePaths))\n",
        "cnt = 0\n",
        "for imagePath in imagePaths:\n",
        "\t# load the image, pre-process it, and store it in the data list\n",
        "        if (cnt % 500) == 0 :\n",
        "            print(cnt)\n",
        "        if (cnt == 55000):\n",
        "            break\n",
        "        image = cv2.imread(imagePath)\n",
        "        image = cv2.resize(image, (60,80))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = img_to_array(image)\n",
        "        data.append(image)\n",
        "        cnt = cnt + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uspCSE7DyuP"
      },
      "source": [
        "x=np.array(data)\n",
        "x.shape\n",
        "\n",
        "#randomly sample here!\n",
        "np.random.shuffle(x)\n",
        "train_images, test_images = np.split(x,[400])    #18000 is the train_size\n",
        "#change the number of channels to 1.\n",
        "\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "train_images /= 255.\n",
        "test_images /= 255.\n",
        "\n",
        "batch_size = 64\n",
        "train_size = len(train_images)\n",
        "test_size = len(x) - train_size\n",
        "\n",
        "#convert to tensorflow type tf.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_size).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(test_size).batch(batch_size)\n",
        "train_dataset\n",
        "#create a write\n",
        "writer = tf.summary.create_file_writer('./new_loss')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQBdYuYeDyw1"
      },
      "source": [
        "#The Convolutional Variational Auto-Encoder Proper. \n",
        "#Define the network class\n",
        "\n",
        "class CVAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):   #latent_dim is the dimensional of the latent space\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.inference_net = tf.keras.Sequential(\n",
        "          [\n",
        "              tf.keras.layers.InputLayer(input_shape=(80,60, 3)),   #Note Input Image Size\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=8, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=8, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=16, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=16, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=32, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=32, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=64, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=64, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Flatten(),\n",
        "              # No activation\n",
        "              tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "          ]\n",
        "        )\n",
        "\n",
        "        self.generative_net = tf.keras.Sequential(\n",
        "            [\n",
        "              tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "              tf.keras.layers.Dense(units=40*30*32, activation=tf.nn.relu),\n",
        "              tf.keras.layers.Reshape(target_shape=(40,30, 32)), #Note the strides formula!\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=8,\n",
        "                  kernel_size=3,\n",
        "                  strides=(2, 2),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=8,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=16,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=16,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              # No activation\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=3, kernel_size= 3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(64, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):   #encode the images into the latent space\n",
        "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    #the reparametrization trick suggests that we randomly sample 'e' from a unit Gaussian,\n",
        "    #and then shift the randomly sampled 'e' by the latent distribution's mean 'mu' and scale it \n",
        "    #by the latent distribution's variance 'rho'.\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):  #decodes the latent space into the image\n",
        "        logits = self.generative_net(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2K8us7rDy0A"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "\n",
        "    kl_tolerance = 0.5\n",
        "    #import pdb; pdb.set_trace()\n",
        "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_logit), axis=[1, 2, 3]))\n",
        "    kl_loss = - 0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar),axis=1)\n",
        "    kl_loss = tf.reduce_mean(tf.maximum(kl_loss, kl_tolerance * latent_dim))\n",
        "    loss_sum = reconstruction_loss + kl_loss\n",
        "    \n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar('Graph of' + '_reconstruction_loss', reconstruction_loss, step=epoch)\n",
        "        tf.summary.scalar('Graph of' + '_kl_loss', kl_loss, step=epoch)\n",
        "        tf.summary.scalar('Graph of' + '_loss_sum',loss_sum, step=epoch)\n",
        "        writer.flush()\n",
        "    return loss_sum, reconstruction_loss, kl_loss\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, r_loss, kl_loss = compute_loss(model, x)\n",
        "        \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return r_loss, kl_loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtFQc4CzDy2U"
      },
      "source": [
        "epochs = 100\n",
        "latent_dim = 128\n",
        "num_examples_to_generate = 12\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(shape=[num_examples_to_generate, latent_dim])\n",
        "\n",
        "model = CVAE(latent_dim)\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model.sample(test_input)\n",
        "    fig = plt.figure(figsize=(16,16))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0])\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    plt.savefig('./images/image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "def test_image(latent_dim,test_img):\n",
        "    test_img = test_img.reshape(1,80,60,3)   #Note the Image size here\n",
        "    model = CVAE(latent_dim)\n",
        "    encod, logvar = model.encode(test_img)\n",
        "    repar = model.reparameterize(encod, logvar)\n",
        "    img = model.decode(repar)\n",
        "    img *= 255.\n",
        "    img = np.squeeze(img)\n",
        "    return plt.imshow(img)\n",
        "\n",
        "generate_and_save_images(model, 0, random_vector_for_generation)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "losslist = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    start_time = time.time()\n",
        "    for train_x in train_dataset:\n",
        "        r_loss, kl_loss = compute_apply_gradients(model, train_x, optimizer)\n",
        "        #print(r_loss, kl_loss)\n",
        "    end_time = time.time()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        #loss = tf.keras.metrics.Mean()\n",
        "        for test_x in test_dataset:\n",
        "            loss_sum, rec_loss, kl_loss = compute_loss(model, test_x)\n",
        "        losslist.append({'loss_sum':loss_sum.numpy(),'reconstruction_loss':rec_loss.numpy(),'kl_loss':kl_loss.numpy()})#append loss to list\n",
        "        \n",
        "        display.clear_output(wait=False)\n",
        "        \n",
        "        df = pd.DataFrame(losslist)  #append the dictionary to the dataframe\n",
        "        df.to_csv('loss.csv')    #correct this\n",
        "\n",
        "        print('Epoch: {}, Test set LOSS_sum: {}, reconstruction_loss: {}, kl_loss: {},'\n",
        "              'time elapse for current epoch {}'.format(epoch,\n",
        "                                                    loss_sum,rec_loss,kl_loss,\n",
        "                                                    end_time - start_time))\n",
        "    model.save_weights('my_model_weights.h5')\n",
        "    generate_and_save_images(\n",
        "        model, epoch, random_vector_for_generation)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzzfRiU2OYgx"
      },
      "source": [
        "def display_image(epoch_no):\n",
        "    return PIL.Image.open('./images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "plt.imshow(display_image(epochs))\n",
        "plt.axis('off')# Display images    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pEwEPmtOZXY"
      },
      "source": [
        "!cp -r event others\n",
        "df = pd.read_csv('loss.csv',encoding = 'unicode_escape')\n",
        "df = df.tail(300)\n",
        "\n",
        "print(df)\n",
        "df.plot(y='reconstruction_loss')\n",
        "#import matplotlib.pyploy as plt\n",
        "\n",
        "df.plot(y='kl_loss')\n",
        "df.plot(y='loss_sum')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuqsqf6OZaI"
      },
      "source": [
        "#The Convolutional Variational Auto-Encoder Proper. \n",
        "#Define the network class\n",
        "\n",
        "class CVAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):   #latent_dim is the dimensional of the latent space\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.inference_net = tf.keras.Sequential(\n",
        "          [\n",
        "              tf.keras.layers.InputLayer(input_shape=(80,60, 3)),   #Note Input Image Size\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=8, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=8, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=16, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=16, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=32, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=32, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=64, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Conv2D(\n",
        "                  filters=64, kernel_size=3, strides=(1, 1), activation='relu'),\n",
        "              tf.keras.layers.Flatten(),\n",
        "              # No activation\n",
        "              tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "          ]\n",
        "        )\n",
        "\n",
        "        self.generative_net = tf.keras.Sequential(\n",
        "            [\n",
        "              tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "              tf.keras.layers.Dense(units=40*30*32, activation=tf.nn.relu),\n",
        "              tf.keras.layers.Reshape(target_shape=(40,30, 32)), #Note the strides formula!\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=8,\n",
        "                  kernel_size=3,\n",
        "                  strides=(2, 2),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=8,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=16,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=16,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  strides=(1, 1),\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              # No activation\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=3, kernel_size= 3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'),\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(64, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):   #encode the images into the latent space\n",
        "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    #the reparametrization trick suggests that we randomly sample 'e' from a unit Gaussian,\n",
        "    #and then shift the randomly sampled 'e' by the latent distribution's mean 'mu' and scale it \n",
        "    #by the latent distribution's variance 'rho'.\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):  #decodes the latent space into the image\n",
        "        logits = self.generative_net(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2q9sWB9OZcr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "\n",
        "    kl_tolerance = 0.5\n",
        "    #import pdb; pdb.set_trace()\n",
        "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_logit), axis=[1, 2, 3]))\n",
        "    kl_loss = - 0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar),axis=1)\n",
        "    kl_loss = tf.reduce_mean(tf.maximum(kl_loss, kl_tolerance * latent_dim))\n",
        "    loss_sum = reconstruction_loss + kl_loss\n",
        "    \n",
        "#     with writer.as_default():\n",
        "#         tf.summary.scalar('Graph of' + '_reconstruction_loss', reconstruction_loss, step=epoch)\n",
        "#         tf.summary.scalar('Graph of' + '_kl_loss', kl_loss, step=epoch)\n",
        "#         tf.summary.scalar('Graph of' + '_loss_sum',loss_sum, step=epoch)\n",
        "#         writer.flush()\n",
        "    return loss_sum, reconstruction_loss, kl_loss\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, r_loss, kl_loss = compute_loss(model, x)\n",
        "        \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return r_loss, kl_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4SFr1JQOZff"
      },
      "source": [
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_examples_to_generate = 12\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(shape=[num_examples_to_generate, latent_dim])\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model.sample(test_input)\n",
        "    fig = plt.figure(figsize=(16,16))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0])\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    plt.savefig('./images/image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jutMoAMKOrgB"
      },
      "source": [
        "add = './Others/Others__1__0.jpg' \n",
        "test_inputs = Image.open(add)\n",
        "newsize = (60, 80) \n",
        "test_inputs = test_inputs.resize(newsize) \n",
        "test_inputs = np.array(test_inputs)   \n",
        "test_inputs = test_inputs.astype('float32')\n",
        "test_inputs /= 255.\n",
        "\n",
        "\n",
        "test_inputs = test_inputs.reshape(1,80,60,3)\n",
        "\n",
        "#from tf.keras.models import load_model\n",
        "\n",
        "def check_image(image,latent_dim):\n",
        "    loaded_model = model\n",
        "    #sample = model.sample()\n",
        "    encod, logvar = loaded_model.encode(image)\n",
        "    repar = loaded_model.reparameterize(encod, logvar)\n",
        "    img = loaded_model.decode(repar)\n",
        "    loss_sum, kl_loss, recon_loss = compute_loss(loaded_model, test_inputs)\n",
        "    loss_sum, kl_loss, recon_loss = loss_sum.numpy(),recon_loss.numpy(),kl_loss.numpy()\n",
        "    print(loss_sum, kl_loss, recon_loss)     #here for a group of pictures, we may want to \n",
        "    img = np.squeeze(img)\n",
        "    return plt.imshow(img)\n",
        "\n",
        "img = check_image(test_inputs,256)\n",
        "\n",
        "img = check_image(test_inputs,256)\n",
        "plt.axis('off')\n",
        "plt.savefig('generative_example',quality = 95)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olt7vFhqOyOg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23U_aF10Oy-2"
      },
      "source": [
        "folder = glob.glob('./Others/*')# test-image folder\n",
        "loss_df = pd.DataFrame()\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "def image_check_resize(folder,latent_dim):\n",
        "    for file in folder:\n",
        "        li = file.split('/')[2]\n",
        "        name = li.split('.')[0]\n",
        "        with open(file, 'r+b') as f:\n",
        "            with Image.open(f) as image:\n",
        "                cover = image.resize((60,80))\n",
        "                test_inputs = np.array(cover)   \n",
        "                test_inputs = test_inputs.astype('float32')\n",
        "                test_inputs /= 255.\n",
        "                test_inputs = test_inputs.reshape(1,80,60,3)\n",
        "                loaded_model = model\n",
        "                encod, logvar = loaded_model.encode(test_inputs)\n",
        "                repar = loaded_model.reparameterize(encod, logvar)\n",
        "                img = loaded_model.decode(repar)\n",
        "                loss_sum, kl_loss, recon_loss = compute_loss(loaded_model, test_inputs)\n",
        "                loss_list.append({'loss_sum':loss_sum.numpy(),'reconstruction_loss':recon_loss.numpy(),'kl_loss':kl_loss.numpy()}) #append loss to list   \n",
        "                df = loss_df.append(loss_list, ignore_index=True)\n",
        "                img = np.squeeze(img)\n",
        "                im = Image.fromarray(np.uint8((img)*255))\n",
        "                im = im.resize((640, 360))\n",
        "                im.save('./others/{}.jpg'.format(name), image.format)\n",
        "                df.to_csv('loss_recon_more_nft.csv')\n",
        "    return plt.imshow(im)\n",
        "\n",
        "show = image_check_resize(folder,latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TKpOJgVO0Hl"
      },
      "source": [
        "folder = glob.glob('./Soccer/*')# test-image folder\n",
        "loss_df = pd.DataFrame()\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "def image_check_resize(folder,latent_dim):\n",
        "    for file in folder:\n",
        "        li = file.split('/')[2]\n",
        "        name = li.split('.')[0]\n",
        "        with open(file, 'r+b') as f:\n",
        "            with Image.open(f) as image:\n",
        "                cover = image.resize((60,80))\n",
        "                test_inputs = np.array(cover)   \n",
        "                test_inputs = test_inputs.astype('float32')\n",
        "                test_inputs /= 255.\n",
        "                test_inputs = test_inputs.reshape(1,80,60,3)\n",
        "                loaded_model = model\n",
        "                encod, logvar = loaded_model.encode(test_inputs)\n",
        "                repar = loaded_model.reparameterize(encod, logvar)\n",
        "                img = loaded_model.decode(repar)\n",
        "                loss_sum, kl_loss, recon_loss = compute_loss(loaded_model, test_inputs)\n",
        "                loss_list.append({'loss_sum':loss_sum.numpy(),'reconstruction_loss':recon_loss.numpy(),'kl_loss':kl_loss.numpy()}) #append loss to list   \n",
        "                df = loss_df.append(loss_list, ignore_index=True)\n",
        "                img = np.squeeze(img)\n",
        "                im = Image.fromarray(np.uint8((img)*255))\n",
        "                im = im.resize((640, 360))\n",
        "                im.save('./others/{}.jpg'.format(name), image.format)\n",
        "                df.to_csv('loss_recon_more_aft.csv')\n",
        "    return plt.imshow(im)\n",
        "\n",
        "show = image_check_resize(folder,latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWMNhGs2O1Tt"
      },
      "source": [
        "folder = glob.glob('./SoccerEvent/*')# test-image folder\n",
        "loss_df = pd.DataFrame()\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "def image_check_resize(folder,latent_dim):\n",
        "    for file in folder:\n",
        "        li = file.split('/')[2]\n",
        "        name = li.split('.')[0]\n",
        "        with open(file, 'r+b') as f:\n",
        "            with Image.open(f) as image:\n",
        "                cover = image.resize((60,80))\n",
        "                test_inputs = np.array(cover)   \n",
        "                test_inputs = test_inputs.astype('float32')\n",
        "                test_inputs /= 255.\n",
        "                test_inputs = test_inputs.reshape(1,80,60,3)\n",
        "                loaded_model = model\n",
        "                encod, logvar = loaded_model.encode(test_inputs)\n",
        "                repar = loaded_model.reparameterize(encod, logvar)\n",
        "                img = loaded_model.decode(repar)\n",
        "                loss_sum, kl_loss, recon_loss = compute_loss(loaded_model, test_inputs)\n",
        "                loss_list.append({'loss_sum':loss_sum.numpy(),'reconstruction_loss':recon_loss.numpy(),'kl_loss':kl_loss.numpy()}) #append loss to list   \n",
        "                df = loss_df.append(loss_list, ignore_index=True)\n",
        "                img = np.squeeze(img)\n",
        "                im = Image.fromarray(np.uint8((img)*255))\n",
        "                im = im.resize((640, 360))\n",
        "                im.save('./others/{}.jpg'.format(name), image.format)\n",
        "                df.to_csv('loss_recon_more_ft.csv')\n",
        "    return plt.imshow(im)\n",
        "\n",
        "show = image_check_resize(folder,latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4FkA1xWO3qp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeBB89nRO2bO"
      },
      "source": [
        "df = pd.read_csv('loss_recon_more_ft.csv')\n",
        "df1 = pd.read_csv('loss_recon_more_nft.csv')\n",
        "df2 = pd.read_csv('loss_recon_more_aft.csv')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mETpeuZO3Gf"
      },
      "source": [
        "ig1 = plt.figure(figsize=(8,8))\n",
        "ax = plt.scatter(y = df['reconstruction_loss'], x = df['kl_loss'], color='b')  #football\n",
        "ax1 = plt.scatter(y = df1['reconstruction_loss'], x = df1['kl_loss'], color='r')  #not_football\n",
        "ax3 = plt.scatter(y = df2['reconstruction_loss'], x = df2['kl_loss'], color='g')  #american_football\n",
        "plt.axhline(y=129, color='r', linestyle='--')\n",
        "plt.legend((ax,ax1,ax3),('football','not_football','american_football'))\n",
        "plt.ylabel('reconstruction_loss')\n",
        "plt.xlabel('kl_loss')\n",
        "plt.ylim((100,200))\n",
        "#plt.savefig('various_rec_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW605raq-Haj"
      },
      "source": [
        "cnt = 0\n",
        "for i in range(0,1200):\n",
        "  if (df['reconstruction_loss'][i] ) < 128 :\n",
        "    cnt = cnt + 1\n",
        "print(cnt)\n",
        "cnt = 0\n",
        "for i in range(0,1200):\n",
        "  if (df1['reconstruction_loss'][i] ) < 128 :\n",
        "    cnt = cnt + 1\n",
        "print(cnt)\n",
        "cnt = 0\n",
        "for i in range(0,1200):\n",
        "  if (df2['reconstruction_loss'][i] ) < 128 :\n",
        "    cnt = cnt + 1\n",
        "print(cnt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}