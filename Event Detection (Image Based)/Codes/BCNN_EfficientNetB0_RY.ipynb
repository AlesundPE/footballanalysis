{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCNN_EfficientNetB0_RY.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qq7XHqnzp5w"
      },
      "source": [
        "import os\n",
        "save_directory= \"Model\"\n",
        "train_address = \"Train\"\n",
        "test_address = \"Test\"\n",
        "\n",
        "if not os.path.exists(test_address):\n",
        "  os.makedirs(test_address)\n",
        "  !wget -q https://raw.githubusercontent.com/circulosmeos/gdown.pl/master/gdown.pl -O gdown.pl\n",
        "  !pip install efficientnet\n",
        "  !perl gdown.pl \"https://drive.google.com/u/0/uc?export=download&confirm=n51E&id=1rxlpshNKP2gQ6NP9Wpvq-jUdbk5t6-Ub\" \"Red-Cards.zip\"\n",
        "  !unzip Red-Cards.zip\n",
        "  !mv Red-Cards Test/Red-Cards\n",
        "  !perl gdown.pl \"https://drive.google.com/u/0/uc?export=download&confirm=Ez4P&id=1tvWzG3stQaY6fqs8O6F_3Ec_qguZU3mC\" \"Yellow-Cards.zip\"\n",
        "  !unzip Yellow-Cards.zip\n",
        "  !mv Yellow-Cards Test/Yellow-Cards\n",
        "\n",
        "if not os.path.exists(train_address):\n",
        "  os.makedirs(train_address)\n",
        "  !perl gdown.pl \"https://drive.google.com/u/0/uc?export=download&confirm=nJ31&id=1sQuc6JoE5CoKq6JYe6i75AoWEBngAVb8\" \"Red-Cards.zip\"\n",
        "  !unzip Red-Cards.zip\n",
        "  !mv Red-Cards Train/Red-Cards\n",
        "  !perl gdown.pl \"https://drive.google.com/u/0/uc?export=download&confirm=vrjS&id=1IozEkL1xhRoG9oqnBvdozrlTcTsqk9s6\" \"Yellow-Cards.zip\"\n",
        "  !unzip Yellow-Cards.zip\n",
        "  !mv Yellow-Cards Train/Yellow-Cards\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl6jAT5Xz5hG"
      },
      "source": [
        "!pip install efficientnet\n",
        "import efficientnet.keras as enet\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Multiply, add\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Input, Reshape, Dense, Lambda, Activation\n",
        "from keras.layers import Input, Reshape, Dense, Lambda, Activation\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.initializers import *\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pandas as pd\n",
        "import os\n",
        "from keras.optimizers import Adam,RMSprop, SGD , Nadam, Adadelta\n",
        "def _outer_product(x):\n",
        "\n",
        "    return K.batch_dot(x[0], x[1], axes=[1, 1]) / x[0].get_shape().as_list()[1]\n",
        "\n",
        "def _signed_sqrt(x):\n",
        "\n",
        "    return K.sign(x) * K.sqrt(K.abs(x) + 1e-9)\n",
        "\n",
        "def _l2_normalize(x, axis=-1):\n",
        "\n",
        "    return K.l2_normalize(x, axis=axis)\n",
        "def buil_bcnn(\n",
        "        all_trainable=False,\n",
        "\n",
        "        size_height=448,\n",
        "        size_width=448,\n",
        "        no_class=200,\n",
        "        no_last_layer_backbone=17,\n",
        "\n",
        "        name_optimizer='sgd',\n",
        "        learning_rate=1.0,\n",
        "        decay_learning_rate=0.0,\n",
        "        decay_weight_rate=0.0,\n",
        "\n",
        "        name_initializer='glorot_normal',\n",
        "        name_activation='softmax',\n",
        "        name_loss='categorical_crossentropy'\n",
        "    ):\n",
        "\n",
        "    input_tensor = Input(shape=[size_height, size_width, 3])\n",
        "    pre_train_model = enet.EfficientNetB0(weights = \"imagenet\", include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "\n",
        "    for layer in pre_train_model.layers:\n",
        "        layer.trainable = all_trainable\n",
        "\n",
        "\n",
        "\n",
        "    model_detector = pre_train_model\n",
        "    output_detector = model_detector.layers[no_last_layer_backbone].output\n",
        "    shape_detector = model_detector.layers[no_last_layer_backbone].output_shape\n",
        "\n",
        "    model_extractor = pre_train_model\n",
        "    output_extractor = model_extractor.layers[no_last_layer_backbone].output\n",
        "    shape_extractor = model_extractor.layers[no_last_layer_backbone].output_shape\n",
        "\n",
        "    output_detector = Reshape(\n",
        "        [shape_detector[1]*shape_detector[2], shape_detector[-1]])(output_detector)\n",
        "    output_extractor = Reshape(\n",
        "        [shape_extractor[1]*shape_extractor[2], shape_extractor[-1]])(output_extractor)\n",
        "\n",
        "    x = Lambda(_outer_product)([output_detector, output_extractor])\n",
        "    x = Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n",
        "    x = Lambda(_signed_sqrt)(x)\n",
        "    x = Lambda(_l2_normalize)(x)\n",
        "\n",
        "\n",
        "\n",
        "    if name_initializer is not None:\n",
        "        name_initializer = eval(name_initializer+'()')\n",
        "\n",
        "    x = Dense(\n",
        "        units=no_class,\n",
        "        kernel_initializer=name_initializer,\n",
        "        kernel_regularizer=l2(decay_weight_rate))(x)\n",
        "    output_tensor = Activation(name_activation)(x)\n",
        "    model_bcnn = Model(inputs=[input_tensor], outputs=[output_tensor])\n",
        "\n",
        "    if name_optimizer == 'adam':\n",
        "        optimizer = adam(lr=learning_rate, decay=decay_learning_rate)\n",
        "    elif name_optimizer == 'rmsprop':\n",
        "        optimizer = RMSprop(lr=learning_rate, decay=decay_learning_rate)\n",
        "    elif name_optimizer == 'sgd':\n",
        "        optimizer = SGD(lr=learning_rate, decay=decay_learning_rate, momentum=0.9, nesterov=None)\n",
        "    else:\n",
        "        raise RuntimeError('Optimizer should be one of Adam, RMSprop and SGD.')\n",
        "\n",
        "    model_bcnn.compile(loss=name_loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model_bcnn\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr49covEz9d0"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 16\n",
        "test_nb = 1000\n",
        "train_nb = 11000\n",
        "num_classes = 2\n",
        "img_size= 224\n",
        "classes = []\n",
        "\n",
        "train_path = \"Train\"\n",
        "test_path = \"Test\"\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255, \n",
        "                                   zoom_range=[1.0,2.0],\n",
        "                                   rotation_range=90,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        seed = 13)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        seed = 13)\n",
        "\n",
        "model = buil_bcnn(\n",
        "    size_height=224,\n",
        "    size_width=224,\n",
        "    no_class=2)\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=train_nb/ 16 ,\n",
        "                    epochs=60,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=int(test_nb/16),\n",
        "                    verbose=1)  #, checkpointer\n",
        "\n",
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "model.evaluate(validation_generator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pTgR71JWNDu"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"model accuracy\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"model loss\")\n",
        "\n",
        "print(\"[INFO] serializing network...\")\n",
        "model.save(\"model_saved\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}